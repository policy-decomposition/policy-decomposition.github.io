<!DOCTYPE html>
<html>

<head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-MM5PQ9VB6G"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MM5PQ9VB6G');
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Affordance-Centric Policy Decomposition: Generalisable and Sample-Efficient Robot Policy Learning for Multi-Object, Long-Horizon Manipulation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/vlmaps_icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
  <style>
    .img-responsive {
      max-width: 100%;
      height: auto;
    }
    .video-responsive {
      width: 100%;
      height: auto;
    }
  </style>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PFJ2DFW" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 2.5em;">
              Affordance-Centric Policy Decomposition <img src="Images/pi_decom.png" style="height: 0.72em;">: </br>
              <span style="font-size: 0.7em;">Generalisable and Sample Efficient Robot Policy Learning for Multi-Object, Long-Horizon Manipulation</span>
            </h1>



            <section class="hero teaser">
              <div class="container is-max-desktop">
                <div class="hero-body">
                  <video id="teaser" autoplay controls muted loop playsinline height="100%">
                    <source src="Images/main1.mp4"
                            type="video/mp4">
                  </video>
                  <p class="has-text-centered" style="color:gray;">
                    Solving <b>long-horizon, multi-object </b> tasks using the equivalent of only <b>10 full task demonstrations</b>.
                  </p>
                </div>
              </div>
            </section>

            <p style="color:red;">
              Please refer to attached supplementary material (.zip) for more results and details.
            </p>


            <section class="section">
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">                 
                </div>
                <br>
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                      <p>
                        Long-horizon manipulation tasks involving multiple different objects present several challenges for imitation learning, with resulting policies exhibiting poor sample efficiency, generalisation, and modularity. Central to these limitations is the use of images and <em>absolute</em> coordinate systems to capture the state of the world. Without extensive demonstration datasets, these representations constrain the policy to operate over a closed set of spatial locations, intra-category instances, and even task variations. In this paper, we present a method to address these challenges using <em>affordance-centric</em> coordinate frames. By appropriately reorienting this frame and training a state-based policy using this <em>relative</em> coordinate system, we demonstrate that we can not only learn highly sample-efficient manipulation behaviours but also generalise to a wide range of spatial and intra-category object variations. More importantly, we show that this representation allows us to learn independent sub-policies that can be seamlessly composed together to solve complex, long-horizon, multi-object tasks, with the modularity for compositional generalisation to new task variations. We extensively validate our approach on a real-world tea-serving task involving 5 different objects, 13 intra-category object variations, and 7 different sub-tasks exhibiting a vast range of spatial variations, demonstrating our ability to solve the entire long-horizon task with the equivalent of only 10 demonstrations.
                      </p>
                    </div>
                  </div>
                </div>
                <br><br>
                <div class="columns is-centered">
                  <div class="column is-full-width">
                    <h2 class="title is-3" style="text-align: left;">Overview</h2>
                    <div class="content has-text-justified">
                      <p style="text-align:center;">
                        <img src="Images/main.png" class="img-responsive">
                      </p>

                      <p class="has-text-centered" style="color:gray;">
                        <span class="dnerf" >Affordance-Centric Policy Decomposition <img src="Images/pi_decom.png" style="height: 0.72em;"></span> factorises <b>multi-object</b>, <b>long-horizon</b> manipulation tasks into a series of sub-policies trained with respect to an affordance-centric task frame. The relative task frame allows us to learn policies that are spatially invariant, while the specifc placement of the frames at a task-relevant affordance-centric region on an object allows for intra-category invariance. Each diffusion policy is trained to additionally predict <b>self-progress</b> across a sub-task enabling it to autonomously transition between sub-tasks in order to complete longer-horizon tasks.
                      </p>
                      <p>
                      </p>

                        <!-- <h6> Affordance-Centric Policy Decomposition </h6>

                          <p>In order to learn sample effient, and generalisable policies for long-horizon, multi-object manipulation tasks, we propose a novel method that leverages <b>affordance-centric task-frames</b>. We decompose policy learning for such tasks into a series of sub-policies each of which is trained to solve a sub-task with respect to an affordance-centric task-frame. To improve data efficiency, we re-orient this frame such that it's <b>funnel axis</b> always points towards the current tool-frame of the robot ensuring the robot consistently operates within the data support of the subsequent sub-policy. The relative task frame allows us to learn policies that are spatially invariant, while the specifc placement of the frames at a task-relevant affordance-centric region on an object allows for intra-category invariance. Each sub-policy is trained using Diffusion Policy from only 10 demonstrations using a fixed set of object instances.</p>

                          <h6> Affordance-Centric Policy Chaining </h6>
                          
                          <p> To autonomously chain the sub-policies to solve longer-horizon tasks, we introduce the concept of <b>self-progress</b> to each sub-policy's action-space. This allows the sub-policy to autonomously transition to the next sub-policy upon completion of the current sub-task. This eliminates the need for a learned arbitrator and allows for seamless composition of sub-policies to solve complex, long-horizon, multi-object tasks.</p> -->
                                                
                      
                    </div>
                    <br/>

                    <h2 class="title is-small" style="text-align: left; font-size: 1.8  rem;" >Results</h2>


                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Long-Horizon Manipulation</h2>

                    
                   
                    <div class="content has-text-justified">
                      <p style="text-align:center;">
                        <img src="Images/task_composition.png" class="img-responsive">
                    
                        <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                          We demonstrate the ability to learn complex long-horizon, multi-object tasks involving prehensile and non-prehensile manipulation actions such as scooping, pouring and pushing. The resulting spatial invariance allows for compositional generalisation to a wide range of object and inter-object spatial variations when solving multi-object tasks.
                        </p>

                      </p>
                    </div>



                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Spatial Generalisation</h2>
                    <div class="content has-text-justified">
                      <p style="text-align:center;">
                        <img src="Images/spatial.png" class="img-responsive">
                      </p>
                    </div>

                    <div class="columns is-centered">
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/spatial_1.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/spatial_2.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/spatial_3.mp4" type="video/mp4">
                        </video>
                      </div>
                      
                    </div>

                    <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                      All policies are trained using demonstrations collected within a small workspace. We demonstrate the ability to generalise to a wide range of spatial variations beyond this training workspace, including object and intra-object placement variations.  
                    </p>

                    <br>

                    


                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Intra-Category Generalisation</h2>
                    <div class="content has-text-justified">
                      <p style="text-align:center;">
                        <img src="Images/intra_cat.png" class="img-responsive">
                      </p>
                    </div>



                    

                    <div class="columns is-centered">
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/teapot_floral.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/cup_pink.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/cup_blue.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/red_teapot.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/cup_tasmania.mp4" type="video/mp4">
                        </video>
                      </div>
                        <div class="column">
                          <video class="video-responsive" autoplay controls muted loop playsinline>
                            <source src="Images/red_cup.mp4" type="video/mp4">
                          </video>
                        </div>
                    </div>


                  
    
                    <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                      The specific placement of our affordance centric task frames allows us to capture the important task relevant regions on objects required for manipulation for a wide range of intra-category object variations. This allows us to abstract away from using images and learn state-based policies with the ability to generalise to a wide range of intra-category object variations from the 10 demonstrations collected on a single object category instance.
                    </p>



                    <br>
                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Policy Robustness</h2>

                    <div class="columns is-centered">
                      <div class="column">
                        <p class="has-text-centered" style="color:gray;font-size: 1.0rem;">
                          Perturbations
                        </p>
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/dynamic_final.mp4" type="video/mp4">
                        </video>
                        <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                          By training a closed-loop diffusion policy that operates on tracked affordance centric task frames, our policies can handle dynamic object disturbances during deployment.
                        </p>
                      </div>

                      <div class="column">
                        <p class="has-text-centered" style="color:gray;font-size: 1.0rem;">
                          Distractor Objects
                        </p>
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/distractor_tea_pour.mp4" type="video/mp4">
                        </video>
                        <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                          Our simplified state representation allows us to learn policies that are robust to distractor objects in the environment.
                        </p>
                      </div>
                    </div>
                    <br>
                    <p class="has-text-centered" style="color:gray;font-size: 1.0rem;">
                      Robot Base Movements
                    </p>
                    <div class="columns is-centered">
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/teacup_place_moving_base.mp4" type="video/mp4">
                        </video>
                      </div>

                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/teapot_pour_mobile_base.mp4" type="video/mp4">
                        </video>
                      </div>
                    </div>
                    <p class="has-text-centered" style="color:gray;font-size: 0.9rem; margin-top: -20px;">
                      By training policies with respect to a relative task frame located on objects, we can learn policies that are robust to robot base movements during deployment with applicability to mobile manipulation.
                    </p>
                    
                    <br>

                    
                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Related Works</h2>


                    <div class="columns is-centered">
                      <div class="column">
                        <img src="Images/foundation_pose.gif" alt="Foundation Pose" class="image-responsive">

                      </div>

                      <div class="column">
                        <br>
                        <p class="has-text-justified" style="color:gray;font-size: 0.9rem; margin-top: -20px;">
                          This work is motivated and enabled by the significant progress made in prior works developing generalist vision systems for keypoint extraction and pose tracking.<br> 
                          <br> 

                          Keypoint identification: <a href="https://arxiv.org/abs/1903.06684" target="_blank" style="color:rgb(88, 88, 244);">kPAM</a>, <a href="https://arxiv.org/abs/1806.08756" target="_blank" style="color:rgb(88, 88, 244);">Dense Object Nets</a>, <a href="https://arxiv.org/abs/2112.05814" target="_blank" style="color:rgb(88, 88, 244);">DINO ViT Features</a>, <a href="https://robopil.github.io/d3fields/" target="_blank" style="color:rgb(88, 88, 244);">D3Fields</a><br>

                          <br>

                          Pose estimation and tracking: <a href="https://nvlabs.github.io/FoundationPose/" target="_blank" style="color:rgb(88, 88, 244);">Foundation Pose</a>, <a href="https://bundlesdf.github.io/" target="_blank" style="color:rgb(88, 88, 244);">BundleSDF</a>

                        
                      </div>
                    </div>


                  </div>
                </div>

              </div>
            </section>
          </div>
        </div>
      </div>
    </div>
  </section>
</body>

</html>
