<!DOCTYPE html>
<html>

<head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-MM5PQ9VB6G"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MM5PQ9VB6G');
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Affordance-Centric Policy Decomposition: Generalisable and Sample-Efficient Robot Policy Learning for Multi-Object, Long-Horizon Manipulation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/vlmaps_icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  
  <style>
    .img-responsive {
      max-width: 100%;
      height: auto;
    }
    .video-responsive {
      width: 100%;
      height: auto;
    }

    .gradient-text {
    background: linear-gradient(to right,  red,yellow, cyan, blue);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}

  </style>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PFJ2DFW" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 2.5em;">
              Affordance-Centric Policy Decomposition <img src="Images/pi_decom.png" style="height: 0.72em;">: </br>
              <span style="font-size: 0.7em;">Generalisable and Sample Efficient Robot Policy Learning for Multi-Object, Long-Horizon Manipulation</span>
            </h1>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="Images/paper.pdf"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href=" "
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="#"
                     class="external-link button is-normal is-rounded ">
                    <span class="icon">
                        <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="#"
                     class="external-link button is-normal is-rounded ">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                    </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="#"
                     class="external-link button is-normal is-rounded ">
                    <span class="icon">
                      <i class="fa-brands fa-x-twitter"></i>
                    </span>
                    <span>Thread</span>
                    </a>
              </div>
            </div>

              <br>



              <section class="hero teaser">
                <div class="container is-max-desktop">
                    <div class="hero-body">
                        <!-- First video, spans full width -->
                        <div class="full-width-video" style="margin-bottom: 20px;">
                            <video id="teaser1" autoplay controls muted loop playsinline width="100%">
                                <source src="Images/main1.mp4" type="video/mp4">
                            </video>
                        </div>
            
                        <!-- Two videos in a row underneath -->
                        <div class="two-videos-row" style="display: flex; justify-content: space-between;">
                            <div class="carousel-item" style="flex: 1; margin-right: 10px;">
                                <video id="teaser2" autoplay controls loop playsinline width="100%" style='border: 10px solid #970303;'>
                                    <source src="Images/coffee_making_FINAL_compressed.mp4" type="video/mp4">
                                </video>
                            </div>
                            <div class="carousel-item" style="flex: 1; margin-left: 10px;" >                              
                                <video id="teaser3" autoplay controls muted loop playsinline width="100%" style='border: 10px solid #970303;'>
                                    <source src="Images/shoe_racking_compressed.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                        
                        <p class="has-text-centered" style="color:gray; margin-top: 20px;">
                            Diffusion Policy solving <b>long-horizon, multi-object</b> tasks using the equivalent of only <b>10 full task demonstrations</b>. The colour <span class="gradient-text">gradient</span> of the diffused trajectory represents the predicted <b>self-progress</b> of the sub-policy for the current sub-task. We additionally overlay the current affordance-centric task frame that the sub-policy is operating with respect to.
                        </p>
                    </div>
                </div>
            </section>
            
            
            

  

            <section class="section">
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">                 
                </div>
                <br>
                <div class="columns is-centered has-text-centered">
                  <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                      <p>
                        Long-horizon manipulation tasks involving multiple different objects present several challenges for imitation learning, with resulting policies exhibiting poor sample efficiency, generalisation, and modularity. Central to these limitations is the use of images and <em>absolute</em> coordinate systems to capture the state of the world. Without extensive demonstration datasets, these representations constrain the policy to operate over a closed set of spatial locations, intra-category instances, and even task variations. In this paper, we present a method to address these challenges using <em>affordance-centric</em> coordinate frames. By appropriately reorienting this frame and training a state-based policy using this <em>relative</em> coordinate system, we demonstrate that we can not only learn highly sample-efficient manipulation behaviours but also generalise to a wide range of spatial and intra-category object variations. More importantly, we show that this representation allows us to learn independent sub-policies that can be seamlessly composed together to solve complex, long-horizon, multi-object tasks, with the modularity for compositional generalisation to new task variations. We extensively validate our approach on a real-world tea-serving task involving 5 different objects, 13 intra-category object variations, and 7 different sub-tasks exhibiting a vast range of spatial variations, demonstrating our ability to solve the entire long-horizon task with the equivalent of only 10 demonstrations.
                      </p>
                    </div>
                  </div>
                </div>
                <br><br>
                <div class="columns is-centered">
                  <div class="column is-full-width">
                    <h2 class="title is-3" style="text-align: left;">Overview</h2>
                    <div class="content has-text-justified">
                      <p style="text-align:center;">
                        <img src="Images/main.png" class="img-responsive">
                      </p>

                      <p class="has-text-centered" style="color:gray;">
                        <span class="dnerf" >Affordance-Centric Policy Decomposition <img src="Images/pi_decom.png" style="height: 0.72em;"></span> factorises <b>multi-object</b>, <b>long-horizon</b> manipulation tasks into a series of sub-policies trained with respect to an affordance-centric task frame. The relative task frame allows us to learn policies that are spatially invariant, while the specifc placement of the frames at a task-relevant affordance-centric region on an object allows for intra-category invariance. Each diffusion policy is trained to additionally predict <b>self-progress</b> across a sub-task enabling it to autonomously transition between sub-tasks in order to complete longer-horizon tasks.
                      </p>
                      <p>
                      </p>

                        <!-- <h6> Affordance-Centric Policy Decomposition </h6>

                          <p>In order to learn sample effient, and generalisable policies for long-horizon, multi-object manipulation tasks, we propose a novel method that leverages <b>affordance-centric task-frames</b>. We decompose policy learning for such tasks into a series of sub-policies each of which is trained to solve a sub-task with respect to an affordance-centric task-frame. To improve data efficiency, we re-orient this frame such that it's <b>funnel axis</b> always points towards the current tool-frame of the robot ensuring the robot consistently operates within the data support of the subsequent sub-policy. The relative task frame allows us to learn policies that are spatially invariant, while the specifc placement of the frames at a task-relevant affordance-centric region on an object allows for intra-category invariance. Each sub-policy is trained using Diffusion Policy from only 10 demonstrations using a fixed set of object instances.</p>

                          <h6> Affordance-Centric Policy Chaining </h6>
                          
                          <p> To autonomously chain the sub-policies to solve longer-horizon tasks, we introduce the concept of <b>self-progress</b> to each sub-policy's action-space. This allows the sub-policy to autonomously transition to the next sub-policy upon completion of the current sub-task. This eliminates the need for a learned arbitrator and allows for seamless composition of sub-policies to solve complex, long-horizon, multi-object tasks.</p> -->
                                                
                      
                    </div>
                    <br/>

                    <h2 class="title is-small" style="text-align: left; font-size: 1.8  rem;" >Key Insights</h2>

                    <h2 id="oaf" class="title is-small" style="text-align: left; font-size: 1.2rem;">Oriented Affordance Frame</h2>

                    <div class="columns is-centered">
                      <div class="column">
                      <p class="has-text-justified" style="color:gray;font-size: 0.9rem;">
                        We introduce the <b>oriented affordance task frame</b> for training sample efficient and composable diffusion policies. Using an affordance-centric task frame for policy learning enables spatial and intra-category generalisation. By orienting this task frame towards the tool frame of the robot at the start of each episode, we can concentrate the data support of the sub-policy around a known 'funnel axis'. This allows us to maximise the utility of only 10 demonstrations, with the ability to compose sub-policies to solve longer-horizon tasks by ensuring that the robot consistently operates within the data support of the subsequent sub-policy. Empirical results additionally show that the oriented frame anchors the task frame in tasks where the object is dynamic, preventing the robot from hitting joint limit violations.
                      </p>

                      <br>
                    </div>
                    <div class="column">
                     <img src="Images/oriented_frame_gif_crop.gif" class="img-responsive" style="height: 250px; width: auto;">
                      <br>
                    </div>
                    </div>

                    <h2 id="selfprogress" class="title is-small" style="text-align: left; font-size: 1.2rem;">Policy Self-Progress</h2>


                    <div class="columns is-centered">
                      <div class="column">
                      <p class="has-text-justified" style="color:gray;font-size: 0.9rem;">
                        <br>
                         We introduce a simple introspective mechanism for policy learning based on the idea of predicting <b>self-progress</b> across a sub-task as an additional action output. This allows us to train sub-policies that can autonomously transition between sub-tasks in order to complete longer-horizon tasks without the need to train an additional arbitrator policy. Given the expressive multi-modality of diffusion policies, we found that a simple linspace() operator over the length of each demonstration served as a well-behaved signal for self-progress prediction as shown in the video.
                      </p>

                      <br>
                    </div>
                    <div class="column">
                     <img id="progress" src="Images/progress_ablation.gif" class="img-responsive" style='border: 10px solid #970303;'>
                      <br>
                    </div>
                    </div>



                    <h2 class="title is-small" style="text-align: left; font-size: 1.8  rem;" >Results</h2>
                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Simplifying Data Collection</h2>
                    <div class="columns is-centered">
                      <div class="column">
                      <p class="has-text-justified" style="color:gray;font-size: 0.9rem;">
                        We simplify data collection for long-horizon, multi-object tasks by training simpler, spatially invariant sub-policies. This approach allows us to collect demonstrations from a subset of the full workspace. The spatial invariance and state-based representation enable compositional generalisation to longer horizon tasks with more objects and spatial variations. We use fiducial markers during data collection to track affordance frames on objects which simplifies dataset processing, later replacing them with keypoint detectors and tracking algorithms. <b>Note:</b> All video demonstrations below use a single set of sub-policies trained from just 10 demonstrations with the objects and spatial variations shown on the right.  
                      </p>

                      
                    </div>
                    <div class="column">
                     <img src="Images/data_collection.png" class="img-responsive">
                      <br>
                    </div>
                    </div>




                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Long-Horizon Manipulation</h2>

                    
                   
                    <div class="content has-text-justified">
                      <p style="text-align:center;">
                        <img src="Images/task_composition.png" class="img-responsive">
                    
                        <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                          We demonstrate the ability to compose the resulting sub-policies to solve <b>long-horizon, multi-object</b> tasks involving prehensile and non-prehensile manipulation actions such as scooping, pouring and pushing. The spatial invariance exhibited by each policy allows for generalisation to compositional variations when solving longer-horizon tasks with multiple objects.
                        </p>

                      </p>
                    </div>



                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Spatial Generalisation</h2>
                    <div class="content has-text-justified">
                      <p style="text-align:center;">
                        <img src="Images/spatial.png" class="img-responsive">
                      </p>
                    </div>

                    <div class="columns is-centered">
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/spatial_1.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/spatial_2.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/spatial_3.mp4" type="video/mp4">
                        </video>
                      </div>
                      
                    </div>

                    <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                      All policies are trained using demonstrations collected within a small workspace. We demonstrate the ability to generalise to a wide range of spatial variations beyond this training workspace, including object and intra-object placement variations.  
                    </p>

                    <br>

                    


                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Intra-Category Generalisation</h2>
                    <div class="content has-text-justified">
                      <p style="text-align:center;">
                        <img src="Images/intra_cat.png" class="img-responsive">
                      </p>
                    </div>



                    

                    <div class="columns is-centered">
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/teapot_floral.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/cup_pink.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/cup_blue.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/red_teapot.mp4" type="video/mp4">
                        </video>
                      </div>
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/cup_tasmania.mp4" type="video/mp4">
                        </video>
                      </div>
                        <div class="column">
                          <video class="video-responsive" autoplay controls muted loop playsinline>
                            <source src="Images/red_cup.mp4" type="video/mp4">
                          </video>
                        </div>
                    </div>


                  
    
                    <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                      The specific placement of our affordance centric task frames allows us to capture the important task relevant regions on objects required for manipulation for a wide range of intra-category object variations. This allows us to abstract away from using images and learn state-based policies with the ability to generalise to a wide range of intra-category object variations from the 10 demonstrations collected on a single object category instance.
                    </p>



                    <br>
                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Policy Robustness</h2>

                    <div class="columns is-centered">
                      <div class="column">
                        <p class="has-text-centered" style="color:gray;font-size: 1.0rem;">
                          Perturbations
                        </p>
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/dynamic_final.mp4" type="video/mp4">
                        </video>
                        <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                          By training a closed-loop diffusion policy that operates on tracked affordance centric task frames, our policies can handle dynamic object disturbances during deployment.
                        </p>
                      </div>

                      <div class="column">
                        <p class="has-text-centered" style="color:gray;font-size: 1.0rem;">
                          Distractor Objects
                        </p>
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/distractor_tea_pour.mp4" type="video/mp4">
                        </video>
                        <p class="has-text-centered" style="color:gray;font-size: 0.9rem;">
                          Our simplified state representation allows us to learn policies that are robust to distractor objects in the environment.
                        </p>
                      </div>
                    </div>
                    <br>
                    <p class="has-text-centered" style="color:gray;font-size: 1.0rem;">
                      Robot Base Movements
                    </p>
                    <div class="columns is-centered">
                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/teacup_place_moving_base.mp4" type="video/mp4">
                        </video>
                      </div>

                      <div class="column">
                        <video class="video-responsive" autoplay controls muted loop playsinline>
                          <source src="Images/teapot_pour_mobile_base.mp4" type="video/mp4">
                        </video>
                      </div>
                    </div>
                    <p class="has-text-centered" style="color:gray;font-size: 0.9rem; margin-top: -20px;">
                      By training policies with respect to a relative task frame located on objects, we can learn policies that are robust to robot base movements during deployment with applicability to mobile manipulation.
                    </p>
                    
                    <br>

                    
                    <h2 class="title is-small" style="text-align: left; font-size: 1.2rem;">Related Works</h2>

                    <br>

                    <p class="has-text-justified" style="color:gray;font-size: 0.9rem; margin-top: -20px;">
                      This work is motivated and enabled by the significant progress made in prior works developing generalist vision systems for keypoint extraction, pose tracking and policy learning.<br> </p>

                      <br>


                    <div class="columns is-centered">
                      <div class="column">
                        <br>
                        <img id=foundationpose src="Images/foundation_pose.gif" alt="Foundation Pose" class="image-responsive">

                      </div>

                      <div class="column">
                        <br>
                        <p class="has-text-justified" style="color:gray;font-size: 0.9rem; margin-top: -30px;">

                          Keypoint identification:<br> <a href="https://arxiv.org/abs/1903.06684" target="_blank" style="color:rgb(88, 88, 244);">kPAM</a>, <a href="https://arxiv.org/abs/1806.08756" target="_blank" style="color:rgb(88, 88, 244);">Dense Object Nets</a>, <a href="https://arxiv.org/abs/2112.05814" target="_blank" style="color:rgb(88, 88, 244);">DINO ViT Features</a>, <a href="https://robopil.github.io/d3fields/" target="_blank" style="color:rgb(88, 88, 244);">D3Fields</a><br>

                          <br>

                          Pose estimation and tracking:<br> <a href="https://nvlabs.github.io/FoundationPose/" target="_blank" style="color:rgb(88, 88, 244);">Foundation Pose</a>, <a href="https://bundlesdf.github.io/" target="_blank" style="color:rgb(88, 88, 244);">BundleSDF</a><br>

                          <br>

                          Policy Learning:<br> <a href="https://diffusion-policy.cs.columbia.edu/" target="_blank" style="color:rgb(88, 88, 244);">Diffusion Policy</a>, <a href="https://umi-gripper.github.io/" target="_blank" style="color:rgb(88, 88, 244);">Universal Manipulation Interface</a><br>

                          <br>
                          
                          <b>Left:</b> We leverage off-the-shelf foundation models for affordance frame localisation (DINO-ViT) and pose tracking (Foundation Pose) in our policy learning framework. This allows use to take advantage of the generalisation capabilities exhibited by these models for robot learning. <br>

                        
                      </div>
                    </div>


                  </div>
                </div>

              </div>
            </section>
          </div>
        </div>
      </div>
    </div>
  </section>
</body>

</html>
